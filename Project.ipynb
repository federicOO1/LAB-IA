{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/federicOO1/LAB-IA/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbxH_B8JnEI_"
      },
      "source": [
        "## import librerie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tA4JIva2nAKh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import albumentations\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phyQbQVZ5-QG",
        "outputId": "8347037f-38ea-4a9c-b545-6d86de4bb39c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.9-cp310-cp310-manylinux2014_x86_64.whl (20.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/20.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/20.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/20.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/20.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/20.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/20.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/20.6 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/20.6 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/20.6 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m17.3/20.6 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2023.11.17)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.23.5)\n",
            "Collecting snuggs>=1.4.1 (from rasterio)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.1)\n",
            "Installing collected packages: snuggs, affine, rasterio\n",
            "Successfully installed affine-2.4.0 rasterio-1.3.9 snuggs-1.4.7\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio\n",
        "import rasterio\n",
        "from rasterio.plot import reshape_as_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ewZSTSHInpM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faaaae3f-00e8-42fb-cb59-b75ea561dd5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vU1WqPPcr2Hf"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/PotsdamDataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "EXjC3h0tnQ_U"
      },
      "outputs": [],
      "source": [
        "class PotsdamDataset(Dataset):\n",
        "    def __init__(self, dataset_folder, transform=None):\n",
        "        self.dataset_folder = dataset_folder\n",
        "        self.image_paths = []\n",
        "        self.world_file_paths = []\n",
        "        self.mask_paths = []\n",
        "        self.transform = transform\n",
        "\n",
        "        # Leggi i percorsi delle immagini, dei file .tfw e delle maschere\n",
        "        end_RGBIR_folder = os.listdir(dataset_folder)[0]\n",
        "        end_LABELS_folder = os.listdir(dataset_folder)[1]\n",
        "\n",
        "        RGBIR_folder = data_folder + '/' + end_RGBIR_folder\n",
        "        LABELS_folder = data_folder + '/' + end_LABELS_folder\n",
        "\n",
        "        for file_name in os.listdir(RGBIR_folder):\n",
        "            if file_name.endswith('.tif'):\n",
        "                image_path = os.path.join(RGBIR_folder, file_name)\n",
        "                world_file_path = os.path.join(RGBIR_folder, file_name.replace('.tif', '.tfw'))\n",
        "\n",
        "                if os.path.exists(world_file_path):\n",
        "                    self.image_paths.append(image_path)\n",
        "                    self.world_file_paths.append(world_file_path)\n",
        "\n",
        "\n",
        "        for label_name in os.listdir(LABELS_folder):\n",
        "            mask_path = os.path.join(LABELS_folder, label_name)\n",
        "\n",
        "            if os.path.exists(mask_path):\n",
        "              self.mask_paths.append(mask_path)\n",
        "        self.image_paths.sort()\n",
        "        self.world_file_paths.sort()\n",
        "        self.mask_paths.sort()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def get_image_paths(self, indices):\n",
        "        return [self.image_paths[idx] for idx in indices]\n",
        "\n",
        "    def get_mask_paths(self, indices):\n",
        "        return [self.mask_paths[idx] for idx in indices]\n",
        "\n",
        "    def load_world_file(self, world_file_path):\n",
        "          lines = open(world_file_path).readlines()\n",
        "          try:\n",
        "              parameters = [float(line.strip()) for line in lines if line.strip()]\n",
        "              if len(parameters) == 6:\n",
        "                  return parameters\n",
        "              else:\n",
        "                  raise ValueError(\"Il file .tfw non contiene 6 parametri.\")\n",
        "          except Exception as e:\n",
        "              print(f\"Errore durante la lettura dei parametri di georeferenziazione: {str(e)}\")\n",
        "              return None\n",
        "\n",
        "    def convert_labels_to_tensor(self, rgb_label):\n",
        "      #print(\"Current rgb_label:\",rgb_label)\n",
        "      # Definisci i colori delle classi nella maschera RGB\n",
        "      colors_to_labels = {\n",
        "          (255, 255, 255): 0,  # Impervious surfaces -> Classe 0\n",
        "          (0, 0, 255): 1,  # Building -> Classe 1\n",
        "          (0, 255, 255): 2,  # Low vegetation -> Classe 2\n",
        "          (0, 255, 0): 3,  # Tree -> Classe 3\n",
        "          (255, 255, 0): 4,  # Car -> Classe 4\n",
        "          (255, 0, 0): 5  # Clutter/background -> Classe 5\n",
        "      }\n",
        "      color_image = np.array(rgb_label)\n",
        "\n",
        "      # Rappresenta color_array come un array (6, 3)\n",
        "      color_array = np.array(list(colors_to_labels.keys()),dtype=np.uint8)\n",
        "\n",
        "      # Reshape l'immagine per renderla compatibile con l'operazione di broadcasting\n",
        "      color_image_reshaped = color_image.reshape(-1, 3)\n",
        "\n",
        "      # Calcola le differenze tra i colori nella maschera RGB e i colori delle classi\n",
        "      color_diffs = np.sum(np.abs(color_image_reshaped[:, None, :] - color_array[None, :, :]), axis=-1)\n",
        "\n",
        "      # Trova l'indice del colore più vicino per ciascun pixel\n",
        "      closest_color_indices = np.argmin(color_diffs, axis=-1)\n",
        "\n",
        "      # Mappa gli indici di colore ai valori delle classi\n",
        "      class_label = np.where(np.min(color_diffs, axis=-1) == 0, closest_color_indices, -1)\n",
        "\n",
        "      # Reshape per tornare alle dimensioni dell'immagine originale\n",
        "      class_label = class_label.reshape(rgb_label.shape[1], rgb_label.shape[2])\n",
        "\n",
        "      # Crea il tensore delle etichette di classe\n",
        "      class_label_tensor = torch.tensor(class_label, dtype=torch.long)\n",
        "\n",
        "      return class_label_tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        world_file_path = self.world_file_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "\n",
        "        # Carica l'immagine TIFF utilizzando la libreria rasterio\n",
        "        image = rasterio.open(image_path).read()\n",
        "\n",
        "        # Carica i parametri di georeferenziazione dal file .tfw\n",
        "        world_params = self.load_world_file(world_file_path)\n",
        "\n",
        "        # Carica la maschera con rasterio\n",
        "        mask = rasterio.open(mask_path).read()\n",
        "\n",
        "        # Converti la maschera RGB nel formato appropriato per CrossEntropyLoss\n",
        "        label = self.convert_labels_to_tensor(mask)\n",
        "\n",
        "        image = torch.from_numpy(image)\n",
        "\n",
        "        # Estrai mean e std dalla lista di trasformazione, se presente\n",
        "        if self.transform and isinstance(self.transform, list) and len(self.transform) == 2:\n",
        "            mean, std = self.transform\n",
        "            # Applica la trasformazione di normalizzazione se sono presenti mean e std\n",
        "            image = self.normalize(image, mean, std)\n",
        "\n",
        "\n",
        "        # Restituisci l'immagine, la maschera e i parametri di georeferenziazione\n",
        "        return image, label, world_params\n",
        "\n",
        "    def normalize(self, image, mean=None, std=None):\n",
        "        if mean is not None and std is not None:\n",
        "            # Converte l'immagine in float\n",
        "            image = image.float()\n",
        "\n",
        "            # Normalizza per canale\n",
        "            for c in range(image.size(0)):\n",
        "                image[c] = (image[c] - mean[c]) / std[c]\n",
        "\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YNKK2DbeS0PI"
      },
      "outputs": [],
      "source": [
        "# Definisci il percorso della cartella contenente i dati .tif e .tfw\n",
        "data_folder = \"/content/drive/MyDrive/PotsdamDataset\"\n",
        "\n",
        "# Crea un'istanza del dataset\n",
        "dataset = PotsdamDataset(data_folder)\n",
        "\n",
        "size = len(dataset)\n",
        "train_size = int(0.7 * size)\n",
        "val_size = int(0.15 * size)\n",
        "test_size = int(size - train_size - val_size)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vNfhCmBOx6ys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd50413-3f94-412f-d40d-28002c55a207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/13 [00:00<?, ?it/s]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_6_15_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_4_12_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "  8%|▊         | 1/13 [01:00<12:07, 60.62s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_6_14_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_2_12_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 15%|█▌        | 2/13 [01:40<08:53, 48.46s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_6_7_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_5_11_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 23%|██▎       | 3/13 [02:22<07:33, 45.39s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_6_13_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_4_10_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 31%|███       | 4/13 [03:05<06:40, 44.52s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_5_13_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_3_11_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 38%|███▊      | 5/13 [03:47<05:49, 43.72s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_2_10_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_6_12_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 46%|████▌     | 6/13 [04:27<04:57, 42.47s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_4_11_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_2_13_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 54%|█████▍    | 7/13 [05:09<04:14, 42.35s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_6_10_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_7_8_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 62%|██████▏   | 8/13 [05:52<03:31, 42.35s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_6_8_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_3_12_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 69%|██████▉   | 9/13 [06:32<02:46, 41.72s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_5_15_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_4_14_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 77%|███████▋  | 10/13 [07:16<02:07, 42.41s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_3_14_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_5_12_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 85%|████████▍ | 11/13 [07:57<01:23, 41.82s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_5_10_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_7_13_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            " 92%|█████████▏| 12/13 [08:39<00:41, 41.93s/it]WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_2_11_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_5_14_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "100%|██████████| 13/13 [09:20<00:00, 43.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean per channel: tensor([86.8845, 92.9506, 86.3022, 98.7621])\n",
            "std per channel: tensor([35.9432, 35.8858, 37.4310, 36.1334])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "num_channels = 4  # RGBIR has 4 channels\n",
        "dtype = torch.float32  # Imposta il tipo di dato desiderato\n",
        "\n",
        "# placeholders\n",
        "psum = torch.zeros(num_channels, dtype=dtype)\n",
        "psum_sq = torch.zeros(num_channels, dtype=dtype)\n",
        "count = 0\n",
        "\n",
        "# loop through images\n",
        "for inputs, labels, georeference_info in tqdm(train_loader):\n",
        "    # Converti il tensore in tipo di dato a precisione maggiore\n",
        "    inputs = inputs.to(dtype)\n",
        "\n",
        "    psum += inputs.sum(axis = [0, 2, 3])\n",
        "    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])\n",
        "\n",
        "    # Conteggio dei pixel\n",
        "    count += inputs.size(0) * inputs.size(2) * inputs.size(3)\n",
        "\n",
        "total_mean = psum / count\n",
        "\n",
        "# Calcola la varianza per canale\n",
        "total_var = (psum_sq / count) - (total_mean ** 2)\n",
        "\n",
        "# Calcola la deviazione standard per canale\n",
        "total_std = torch.sqrt(total_var)\n",
        "# output\n",
        "print('mean per channel: ' + str(total_mean))\n",
        "print('std per channel: ' + str(total_std))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "t1NmW-8RojEr"
      },
      "outputs": [],
      "source": [
        "normalize_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=total_mean.tolist(), std=total_std.tolist())])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_with_transform = PotsdamDataset(data_folder, transform=[total_mean, total_std])"
      ],
      "metadata": {
        "id": "KcX3iYUJzGWi"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "HJrIM9aPoUjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9777029f-0a3e-4f56-972d-c48b2e829dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rasterio._env:CPLE_AppDefined in /content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_2_10_RGBIR.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.8314, -0.8314, -0.9427,  ..., -0.2750, -0.4141, -0.5810],\n",
              "         [-1.0262, -0.9984, -0.9705,  ..., -0.3028, -0.3585, -0.6089],\n",
              "         [-1.1653, -1.0540, -1.0262,  ..., -0.2472, -0.3585, -0.6089],\n",
              "         ...,\n",
              "         [ 0.4484,  0.5318,  0.6431,  ...,  0.0589,  0.0589,  0.0589],\n",
              "         [ 0.3649,  0.4484,  0.4205,  ...,  0.0589,  0.0589,  0.0589],\n",
              "         [ 0.3371,  0.4205,  0.4205,  ...,  0.0032,  0.0310,  0.0589]],\n",
              "\n",
              "        [[-0.7510, -0.7510, -0.8903,  ..., -0.2494, -0.3887, -0.6117],\n",
              "         [-1.0018, -0.9182, -0.9461,  ..., -0.2773, -0.3052, -0.6674],\n",
              "         [-1.1969, -0.9739, -0.9461,  ..., -0.1658, -0.3052, -0.6395],\n",
              "         ...,\n",
              "         [ 0.6702,  0.7259,  0.7816,  ...,  0.5308,  0.5308,  0.5587],\n",
              "         [ 0.6144,  0.5866,  0.5030,  ...,  0.5030,  0.5030,  0.5587],\n",
              "         [ 0.5587,  0.5866,  0.4472,  ...,  0.4751,  0.5030,  0.6144]],\n",
              "\n",
              "        [[-0.7027, -0.7027, -0.8363,  ..., -0.5424, -0.7027, -0.8095],\n",
              "         [-0.9966, -0.9431, -0.9431,  ..., -0.5691, -0.6493, -0.9164],\n",
              "         [-1.2370, -1.0233, -0.9698,  ..., -0.5157, -0.6760, -0.8897],\n",
              "         ...,\n",
              "         [ 0.6331,  0.7133,  0.7667,  ...,  0.7667,  0.7934,  0.8201],\n",
              "         [ 0.5530,  0.5262,  0.4461,  ...,  0.7400,  0.7400,  0.8201],\n",
              "         [ 0.5262,  0.5530,  0.4194,  ...,  0.6865,  0.7400,  0.8468]],\n",
              "\n",
              "        [[-0.9344, -0.9067, -0.9897,  ...,  1.0859,  0.8922,  0.6985],\n",
              "         [-1.1004, -1.0174, -1.0174,  ...,  1.0582,  0.9752,  0.5601],\n",
              "         [-1.2388, -1.0727, -0.9897,  ...,  1.1689,  0.9475,  0.6154],\n",
              "         ...,\n",
              "         [ 0.0066,  0.0619,  0.1726,  ..., -0.3255, -0.3532, -0.3532],\n",
              "         [-0.0211,  0.0343,  0.0066,  ..., -0.3532, -0.3809, -0.3255],\n",
              "         [-0.0764, -0.0211, -0.0211,  ..., -0.4085, -0.3809, -0.2978]]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "dataset_with_transform[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DZZiSvIYye-8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "f881c260-7ef3-43d4-be1b-68130a8b5273"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_2_10_RGBIR.tif'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a5186501281c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data_loader_normal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3281\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3283\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/PotsdamDataset/4_Ortho_RGBIR/top_potsdam_2_10_RGBIR.tif'>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementazione UNET"
      ],
      "metadata": {
        "id": "Ie3dAWdRrH96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DoubleConvolution(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConvolution, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, features=[64,128,256,512]):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        for feature in features:\n",
        "          self.downs.append(DoubleConvolution(in_channels, feature))\n",
        "          in_channels = feature\n",
        "\n",
        "        for feature in reversed(features):\n",
        "          self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
        "          self.ups.append(DoubleConvolution(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConvolution(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      skip_connections = []\n",
        "      for down in self.downs:\n",
        "        x = down(x)\n",
        "        skip_connections.append(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "      x = self.bottleneck(x)\n",
        "      skip_connections = skip_connections[::-1]\n",
        "\n",
        "      for i in range(0,len(self.ups),2):\n",
        "        x = self.ups[i](x)\n",
        "        skip_connection = skip_connections[i//2]\n",
        "\n",
        "        if x.shape != skip_connection.shape:\n",
        "          x = transforms.functional.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "        concat_skip = torch.cat((skip_connection,x), dim=1)\n",
        "        x = self.ups[i+1](concat_skip)\n",
        "\n",
        "      return self.final_conv(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "OWb0viXJrL2-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((3,1,160,160))\n",
        "model = UNet(in_channels=1, out_channels=1)\n",
        "preds = model(x)\n",
        "print(preds.shape)\n",
        "print(x.shape)\n",
        "assert preds.shape == x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeWgrMvuDKMV",
        "outputId": "f96249a1-41dc-4c9f-afaf-32ea37062e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 160, 160])\n",
            "torch.Size([3, 1, 160, 160])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training part"
      ],
      "metadata": {
        "id": "l-6du0zqmWb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_function(loader, model, optimizer, loss_function, scaler):\n",
        "  for idx, batch in enumerate(tqdm(loader)):\n",
        "    images, labels, geo_inf = batch\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "      predictions = model(images)\n",
        "      loss = loss_function(predictions, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()"
      ],
      "metadata": {
        "id": "ibKWL8kQUQ5I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  train_transform = albumentations.Compose([\n",
        "      albumentations.Normalize(mean=total_mean, std=total_std, max_pixel_value=255.0),\n",
        "      albumentations.ToTensorV2()\n",
        "  ])\n",
        "  val_transform = albumentations.Compose([\n",
        "      albumentations.Normalize(mean=total_mean, std=total_std, max_pixel_value=255.0),\n",
        "      albumentations.ToTensorV2()\n",
        "  ])\n",
        "  test_transform = albumentations.Compose([\n",
        "      albumentations.Normalize(mean=total_mean, std=total_std, max_pixel_value=255.0),\n",
        "      albumentations.ToTensorV2()\n",
        "  ])\n",
        "  model = UNet(in_channels=4, out_channels=6).to(DEVICE)\n",
        "  loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "sIIPTSCSqv55"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "tnPplxZRs3Pk",
        "outputId": "599dc902-a29d-49fa-864b-78cf93cf80c3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'albumentations' has no attribute 'ToTensorV2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-20a4c88e53ea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-9689449d7520>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   train_transform = albumentations.Compose([\n\u001b[1;32m      3\u001b[0m       \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_pixel_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   ])\n\u001b[1;32m      6\u001b[0m   val_transform = albumentations.Compose([\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'albumentations' has no attribute 'ToTensorV2'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzn3WvGpOmBjiQ1W5h+4yY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}